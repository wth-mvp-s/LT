Plik nr 002 orders

#### 003::
`You have a table named SalesFact in an enterprise data warehouse in Azure Synapse Analytics. SalesFact contains sales data from the past 36 months and has the following characteristics:
`✑ Is partitioned by month
`✑ Contains one billion rows
`✑ Has clustered columnstore index
`At the beginning of each month, you need to remove data from SalesFact that is older than 36 months as quickly as possible.
`Which three actions should you perform in sequence in a stored procedure? 
`Select and Place:

- [0] Create an empty table named SalesFact_Work that has the same schema as SalesFact.
- [1] Switch the partition containing the stale data from SalesFact to SalesFact_Work.
- [2] Drop the SalesFact_Work table.
- [-] Truncate the partition containing the stale data.
- [-] 
`Execute a DELETE statement where the value in the Date column is more than 36 months ago.
- [-] Copy the data to a new table by using CREATE TABLE AS SELECT (CTAS).

#### 0041::
`You need to build a solution to ensure that users can query specific files in an Azure Data Lake Storage Gen2 account from an Azure Synapse Analytics serverless SQL pool.
`Which three actions should you perform in sequence?

- [1] Create an external file format object
- [0] Create an external data source
- [-] Create a query that uses Create Table as Select
- [-] Create a table
- [2] Create an external table

#### 0044::
`You have data stored in thousands of CSV files in Azure Data Lake Storage Gen2. Each file has a header row followed by a properly formatted carriage return (/ r) and line feed (/n).
`You are implementing a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics by using PolyBase.
`You need to skip the header row when you import the files into the data warehouse. Before building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics.
`Which three actions should you perform in sequence?

- [-] 
`Create a database scoped credential that uses Azure Active Directory Application and a Service Principal Key
- [0]
`Create an external data source that uses the abfs location
- [2] 
`Use CREATE EXTERNAL TABLE AS SELECT
`(CETAS) and configure the reject options to
`specify reject values or percentages
- [1] 
`Create an external file format and set the
`First_Row option







#### 0021::
`You have an Azure Data Lake Storage Gen2 account that contains a JSON file for customers. The file contains two attributes named FirstName and LastName.
`You need to copy the data from the JSON file to an Azure Synapse Analytics table by using Azure Databricks. A new column must be created that concatenates the FirstName and LastName values.
`You create the following components:
`✑ A destination table in Azure Synapse
`✑ An Azure Blob storage container
`✑ A service principal
`Which five actions should you perform in sequence next in is Databricks notebook?
- [0] Mount the Data Lake Storage onto DBF:
- [4] Write the results to a table in Azure Synapse.
- [-] Perform transformations on the file.
- [3] Specify a temporary folder to stage the data.
- [-] Write the results to Data Lake Storage.
- [1] Read the file into a data frame.
- [-] Drop the data frame.
- [2] Perform transformations on the data frame.

#### 0024::
`You have an Azure Stream Analytics job that is a Stream Analytics project solution in Microsoft Visual Studio.
`The job accepts data generated by IoT devices in the JSON format.
`You need to modify the job to accept data generated by the IoT devices in the Protobuf format.
`Which three actions should you perform from Visual Studio on sequence?

- [0] 
`Add an Azure Stream Analytics Custom Deserializer
`Project (.NET) project to the solution.
- [1] 
`Add .NET deserializer code for Protobuf to the
`custom deserializer project.
- [2] 
`Add an Azure Stream Analytics Application project
`to the solution
- [-] 
`Change the Event Serialization Format to Protobuf in
`the input json file of the job and reference the DLL.
- [-] 
`Add .NET deserializer code for Protobuf to the
`Stream

#### 0046::
`You are responsible for providing access to an Azure Data Lake Storage Gen2 account.
`Your user account has contributor access to the storage account, and you have the application ID and access key.
`You plan to use PolyBase to load data into an enterprise data warehouse in Azure Synapse Analytics.
`You need to configure PolyBase to connect the data warehouse to storage account.
`Which three components should you create in sequence?

- [0] an asymmetric key
- [1] a database scoped credential
- [2] an external data source
- [-] a database encryption key
- [-] an external file format








#### 0077::
`You have an Azure Data Lake Storage Gen2 account that contains a JSON file for customers. The file contains two attributes named FirstName and LastName.
`You need to copy the data from the JSON file to an Azure Synapse Analytics table by using Azure Databricks. A new column must be created that concatenates the FirstName and LastName values.
`You create the following components:
`✑ A destination table in Azure Synapse
`✑ An Azure Blob storage container
`✑ A service principal
`In which order should you perform the actions? 
- [0] Mount the Data Lake Storage onto DBFS.
- [4] Write the results to a table in Azure Synapse.
- [3] Specify a temporary folder to stage the data.
- [1] Read the file into a data frame.
- [2] Perform transformations on the data frame.

#### 0079::
`You have an Azure subscription that contains an Azure Synapse Analytics workspace named workspace1. Workspace1 connects to an Azure DevOps repository named repo1. Repo1 contains a collaboration branch named main and a development branch named branch1. Branch1 contains an Azure Synapse pipeline named pipeline1.
`In workspace1, you complete testing of pipeline1.
`You need to schedule pipeline1 to run daily at 6 AM.
`Which four actions should you perform in sequence?
- [-] Create a new branch in Repo.
- [2] Merge the changes from branch into main.
- [1] Associate the schedule trigger with pipelined.
- [-] Switch to Synapse live mode.
- [0] Create a schedule trigger.
- [3] Publish the contents of main.

#### 0095::
`You have a project in Azure DevOps that contains a repository named Repo1. Repo1 contains a branch named main.
`
`You create a new Azure Synapse workspace named Workspace1.
`
`You need to create data processing pipelines in Workspace1. The solution must meet the following requirements:
`
`• Pipeline artifacts must be stored in Repo1
`• Source control must be provided for pipeline artifacts.
`• All development must be performed in a feature branch.
`
`Which four actions should you perform in sequence in Synapse Studio?
- [-] Create pipeline artifacts and save them in the main branch.
- [-] Set the main branch as the collaboration branch.
- [3] Create a pull request to merge the contests of the main branch into the new branch.
- [2] Create pipeline artifacts and save them in the new branch.
- [1] Create a new branch.
- [0] Configure a code repository and select Repo 1.

#### 001::
`You have an Azure Active Directory (Azure AD) tenant that contains a security group named Group1. You have an Azure Synapse Analytics dedicated SQL pool named dw1 that contains a schema named schema1.
`You need to grant Group1 read-only permissions to all the tables and views in schema1. The solution must use the principle of least privilege.
`Which three actions should you perform in sequence?
- [1] Create a database role named Rolel and grant Rolel
`SELECT permissions to schemal.
- [-] Create a database role named Rolel and grant Rolel
`SELECT permission to dw1
- [-] Assign the Azure role-based access control (Azure
`RBAC) Reader role for dw1 to Group1.
- [0] Create a database user in dwi that represents Group1
`and uses the FROM EXTERNAL PROVIDER clause.
- [2] Assign Rolel to the Group1 database user.

#### 0010::
`You have an Azure Synapse Analytics SQL pool named Pool1 on a logical Microsoft SQL server named Server1.
`You need to implement Transparent Data Encryption (TDE) on Pool1 by using a custom key named key1.
`Which five actions should you perform in sequence?
- [4] Enable TDE on Pool1.
- [0] Assign a managed identity to Server1.
- [3] Configure key1 as the TDE protector for Server1.
- [2] Add key1 to the Azure key vault
- [1] Create an Azure key Vault and grant the managed identity permissions to the key vault








#### 0027::
`You have an Azure data factory.
`You need to ensure that pipeline-run data is retained for 120 days. The solution must ensure that you can query the data by using the Kusto query language.
`Which four actions should you perform in sequence?
- [-] 
`Select the PipelineRuns category.
- [1] 
`Create a Log Analytics workspace that
`has Data Retention set to 120 days.
- [-] 
`Stream to an Azure event hub.
- [0] 
`Create an Azure Storage account that
`has a lifecycle policy.
- [2] 
`From the Azure portal, add a
`diagnostic setting.
- [3] 
`Send the data to a Log Analytics
`workspace.
- [-] 
`Select the TriggerRuns category.









#### 002::
`You need to ensure that the Twitter feed data can be analyzed in the dedicated SQL pool. The solution must meet the customer sentiment analytics requirements.
`Which three Transact-SQL DDL commands should you run in sequence?
- [0] CREATE EXTERNAL DATA SOURCE
- [1] CREATE EXTERNAL FILE FORMAT
- [2] CREATE EXTERNAL TABLE AS SELECT
- [-] CREATE DATABASE SCOPED CREDENTIAL
- [-] CREATE EXTERNAL TABLE


#### 001 Topic 7 ::
`You need to implement versioned changes to the integration pipelines. The solution must meet the data integration requirements.
`In which order should you perform the actions?
- [0] Create a repository and a main branch
- [1] Create a feature branch
- [2] Create a pull request
- [3] Merge changes
- [4] Publish changes